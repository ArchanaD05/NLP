{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DA1vLCv_fE5r"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import subjectivity\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from nltk.tokenize import sent_tokenize  # Import sent_tokenize\n",
        "from sklearn.metrics import make_scorer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('vader_lexicon')\n",
        "drive.mount(\"/drive\", force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-OzHRq-nGTu",
        "outputId": "45401abf-882c-4c29-f67a-327daed5c80e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the True and Fake datasets\n",
        "true_df = pd.read_csv('/drive/My Drive/Colab Notebooks/IST 664 2023/True.csv')\n",
        "fake_df = pd.read_csv('/drive/My Drive/Colab Notebooks/IST 664 2023/Fake.csv')"
      ],
      "metadata": {
        "id": "cVWgXrpngx8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1: Analyze Sentences in CSV Files"
      ],
      "metadata": {
        "id": "E4_rpSOCUk36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract first 50 rows\n",
        "true_df = true_df.head(50)\n",
        "fake_df = fake_df.head(50)"
      ],
      "metadata": {
        "id": "mwsiJvz9wb2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to remove special characters from text\n",
        "def remove_special_characters(text):\n",
        "    # Define a regular expression pattern to match special characters\n",
        "    pattern = r'[^\\w\\s]'\n",
        "\n",
        "    # Use the re.sub() function to remove special characters\n",
        "    clean_text = re.sub(pattern, '', text)\n",
        "\n",
        "    return clean_text\n",
        "\n",
        "# Apply the remove_special_characters function to the 'text' column of both datasets\n",
        "true_df['text'] = true_df['text'].apply(remove_special_characters)\n",
        "fake_df['text'] = fake_df['text'].apply(remove_special_characters)\n"
      ],
      "metadata": {
        "id": "UBqTPcd-m82N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function for text preprocessing\n",
        "def preprocess_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = ''.join([char for char in text if char not in string.punctuation])\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Rejoin tokens into a clean text\n",
        "    clean_text = ' '.join(tokens)\n",
        "\n",
        "    return clean_text\n",
        "\n",
        "# Apply the preprocessing function to the text columns\n",
        "true_df['text'] = true_df['text'].apply(preprocess_text)\n",
        "fake_df['text'] = fake_df['text'].apply(preprocess_text)\n",
        "\n",
        "# Now, the 'text' columns in true_df and fake_df are preprocessed and ready for further use."
      ],
      "metadata": {
        "id": "icJXJc5xgM8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add labels to distinguish between true and fake news\n",
        "true_df['label'] = 1  # 1 for true news\n",
        "fake_df['label'] = 0  # 0 for fake news"
      ],
      "metadata": {
        "id": "ZTLzxIofdlrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate the datasets vertically to create a single combined dataset\n",
        "combined_df = pd.concat([true_df, fake_df], ignore_index=True)"
      ],
      "metadata": {
        "id": "bm-pCl4Xdplv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle the data\n",
        "combined_df = combined_df.sample(frac=1, random_state=42)"
      ],
      "metadata": {
        "id": "hYg00a4Cg5j5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You can access the text and labels as follows:\n",
        "X = combined_df['text']  # The text data\n",
        "y = combined_df['label']  # The labels (true/fake)"
      ],
      "metadata": {
        "id": "NuWYQ5wsdsuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2: Build and Evaluate Classifier"
      ],
      "metadata": {
        "id": "pf27zgwLUsSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_feature_set(X, y, feature_set):\n",
        "    if feature_set == \"unigram\":\n",
        "        vectorizer = CountVectorizer()\n",
        "    elif feature_set == \"bigram\":\n",
        "        vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
        "    elif feature_set == \"tfidf\":\n",
        "        vectorizer = TfidfVectorizer()\n",
        "    elif feature_set == \"countvec_with_stopwords\":\n",
        "        vectorizer = CountVectorizer(stop_words='english')\n",
        "    else:\n",
        "        raise ValueError(\"Invalid feature_set. Choose from 'unigram', 'bigram', or 'tfidf' or 'countvec_with_stopwords'.\")\n",
        "\n",
        "    X_vec = vectorizer.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_vec, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Initialize and train a classifier (e.g., Multinomial Naive Bayes)\n",
        "    classifier = MultinomialNB()\n",
        "    classifier.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the test data\n",
        "    y_pred = classifier.predict(X_test)\n",
        "\n",
        "    # Calculate precision, recall, and F1-score\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "    return precision, recall, f1\n",
        "\n",
        "# Usage example:\n",
        "feature_sets = [\"unigram\", \"bigram\", \"tfidf\", \"countvec_with_stopwords\"]\n",
        "for feature_set in feature_sets:\n",
        "    precision, recall, f1 = evaluate_feature_set(X, y, feature_set)\n",
        "    print(f\"Feature Set: {feature_set}\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"F1 Score: {f1:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pV5twlz5WHmp",
        "outputId": "4f41c876-4156-4acc-9674-be86ac8df648"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Set: unigram\n",
            "Precision: 1.00\n",
            "Recall: 1.00\n",
            "F1 Score: 1.00\n",
            "Feature Set: bigram\n",
            "Precision: 1.00\n",
            "Recall: 1.00\n",
            "F1 Score: 1.00\n",
            "Feature Set: tfidf\n",
            "Precision: 1.00\n",
            "Recall: 1.00\n",
            "F1 Score: 1.00\n",
            "Feature Set: countvec_with_stopwords\n",
            "Precision: 1.00\n",
            "Recall: 1.00\n",
            "F1 Score: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom scoring function for f1_score in cross-validation\n",
        "custom_scorer = make_scorer(f1_score)\n",
        "\n",
        "# Continue from the previous step\n",
        "for feature_set in feature_sets:\n",
        "    vectorizer = None  # Reset the vectorizer for each iteration\n",
        "\n",
        "    if feature_set == \"unigram\":\n",
        "        vectorizer = CountVectorizer()\n",
        "    elif feature_set == \"bigram\":\n",
        "        vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
        "    elif feature_set == \"tfidf\":\n",
        "        vectorizer = TfidfVectorizer()\n",
        "    elif feature_set == \"countvec_with_stopwords\":\n",
        "        vectorizer = CountVectorizer(stop_words='english')\n",
        "    else:\n",
        "        raise ValueError(\"Invalid feature_set. Choose from 'unigram', 'bigram', 'tfidf', or 'countvec_with_stopwords'.\")\n",
        "\n",
        "    X_vec = vectorizer.fit_transform(X)\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_vec, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Initialize and train a classifier (e.g., Multinomial Naive Bayes)\n",
        "    classifier = MultinomialNB()\n",
        "    classifier.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the test data\n",
        "    y_pred = classifier.predict(X_test)\n",
        "\n",
        "    # Calculate precision, recall, and F1-score\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "    # Perform cross-validation with custom scoring\n",
        "    cv_scores = cross_val_score(classifier, X_vec, y, cv=5, scoring=custom_scorer)\n",
        "\n",
        "    print(f\"Feature Set: {feature_set}\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"F1 Score: {f1:.2f}\")\n",
        "    print(f\"Cross-Validation F1 Score: {cv_scores.mean():.2f}\")\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvfdzwmxaAt3",
        "outputId": "dc170793-6572-4cd2-b2d7-573192453ed1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Set: unigram\n",
            "Precision: 1.00\n",
            "Recall: 1.00\n",
            "F1 Score: 1.00\n",
            "Cross-Validation F1 Score: 0.96\n",
            "\n",
            "\n",
            "Feature Set: bigram\n",
            "Precision: 1.00\n",
            "Recall: 1.00\n",
            "F1 Score: 1.00\n",
            "Cross-Validation F1 Score: 0.96\n",
            "\n",
            "\n",
            "Feature Set: tfidf\n",
            "Precision: 1.00\n",
            "Recall: 1.00\n",
            "F1 Score: 1.00\n",
            "Cross-Validation F1 Score: 0.96\n",
            "\n",
            "\n",
            "Feature Set: countvec_with_stopwords\n",
            "Precision: 1.00\n",
            "Recall: 1.00\n",
            "F1 Score: 1.00\n",
            "Cross-Validation F1 Score: 0.96\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perfoem Sentiment Analysis on the datasets\n"
      ],
      "metadata": {
        "id": "gXR-3O_0QyFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the VADER Sentiment Analyzer\n",
        "analyzer = SentimentIntensityAnalyzer()"
      ],
      "metadata": {
        "id": "V8c7itQpFgYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to calculate overall sentiment\n",
        "def calculate_overall_sentiment(text):\n",
        "    sentiment = analyzer.polarity_scores(text)\n",
        "    compound_score = sentiment['compound']\n",
        "\n",
        "    if compound_score >= 0.05:\n",
        "        return 'Positive'\n",
        "    elif compound_score <= -0.05:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'"
      ],
      "metadata": {
        "id": "EO11F7Y-bGnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply sentiment analysis to 'text' column of true_data\n",
        "true_df['Sentiment'] = true_df['text'].apply(calculate_overall_sentiment)"
      ],
      "metadata": {
        "id": "qBfc2gz6bH2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply sentiment analysis to 'text' column of fake_data\n",
        "fake_df['Sentiment'] = fake_df['text'].apply(calculate_overall_sentiment)"
      ],
      "metadata": {
        "id": "eDGfnFQobLD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first 20 rows of both datasets with sentiment analysis results\n",
        "print(\"First 20 rows of 'true_data' with sentiment analysis results:\")\n",
        "print(true_df.head(20))\n",
        "\n",
        "print(\"\\nFirst 20 rows of 'fake_data' with sentiment analysis results:\")\n",
        "print(fake_df.head(20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0SP7Oy6bOFz",
        "outputId": "f410e680-d8e1-452e-a9c0-831016a29429"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 20 rows of 'true_data' with sentiment analysis results:\n",
            "                                                title  \\\n",
            "0   As U.S. budget fight looms, Republicans flip t...   \n",
            "1   U.S. military to accept transgender recruits o...   \n",
            "2   Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
            "3   FBI Russia probe helped by Australian diplomat...   \n",
            "4   Trump wants Postal Service to charge 'much mor...   \n",
            "5   White House, Congress prepare for talks on spe...   \n",
            "6   Trump says Russia probe will be fair, but time...   \n",
            "7   Factbox: Trump on Twitter (Dec 29) - Approval ...   \n",
            "8          Trump on Twitter (Dec 28) - Global Warming   \n",
            "9   Alabama official to certify Senator-elect Jone...   \n",
            "10  Jones certified U.S. Senate winner despite Moo...   \n",
            "11  New York governor questions the constitutional...   \n",
            "12  Factbox: Trump on Twitter (Dec 28) - Vanity Fa...   \n",
            "13     Trump on Twitter (Dec 27) - Trump, Iraq, Syria   \n",
            "14  Man says he delivered manure to Mnuchin to pro...   \n",
            "15  Virginia officials postpone lottery drawing to...   \n",
            "16  U.S. lawmakers question businessman at 2016 Tr...   \n",
            "17  Trump on Twitter (Dec 26) - Hillary Clinton, T...   \n",
            "18  U.S. appeals court rejects challenge to Trump ...   \n",
            "19  Treasury Secretary Mnuchin was sent gift-wrapp...   \n",
            "\n",
            "                                                 text       subject  \\\n",
            "0   washington reuters head conservative republica...  politicsNews   \n",
            "1   washington reuters transgender people allowed ...  politicsNews   \n",
            "2   washington reuters special counsel investigati...  politicsNews   \n",
            "3   washington reuters trump campaign adviser geor...  politicsNews   \n",
            "4   seattlewashington reuters president donald tru...  politicsNews   \n",
            "5   west palm beach flawashington reuters white ho...  politicsNews   \n",
            "6   west palm beach fla reuters president donald t...  politicsNews   \n",
            "7   following statements posted verified twitter a...  politicsNews   \n",
            "8   following statements posted verified twitter a...  politicsNews   \n",
            "9   washington reuters alabama secretary state joh...  politicsNews   \n",
            "10  reuters alabama officials thursday certified d...  politicsNews   \n",
            "11  new yorkwashington reuters new us tax code tar...  politicsNews   \n",
            "12  following statements posted verified twitter a...  politicsNews   \n",
            "13  following statements posted verified twitter a...  politicsNews   \n",
            "14  dec 25 story second paragraph corrects name st...  politicsNews   \n",
            "15  reuters lottery drawing settle tied virginia l...  politicsNews   \n",
            "16  washington reuters georgianamerican businessma...  politicsNews   \n",
            "17  following statements posted verified twitter a...  politicsNews   \n",
            "18  reuters us appeals court washington tuesday up...  politicsNews   \n",
            "19  reuters giftwrapped package addressed us treas...  politicsNews   \n",
            "\n",
            "                  date  label Sentiment  \n",
            "0   December 31, 2017       1  Positive  \n",
            "1   December 29, 2017       1  Positive  \n",
            "2   December 31, 2017       1  Positive  \n",
            "3   December 30, 2017       1  Negative  \n",
            "4   December 29, 2017       1  Positive  \n",
            "5   December 29, 2017       1  Positive  \n",
            "6   December 29, 2017       1  Positive  \n",
            "7   December 29, 2017       1  Positive  \n",
            "8   December 29, 2017       1  Positive  \n",
            "9   December 28, 2017       1  Positive  \n",
            "10  December 28, 2017       1  Negative  \n",
            "11  December 28, 2017       1  Positive  \n",
            "12  December 28, 2017       1  Positive  \n",
            "13  December 28, 2017       1  Positive  \n",
            "14  December 25, 2017       1  Negative  \n",
            "15  December 27, 2017       1  Negative  \n",
            "16  December 27, 2017       1  Positive  \n",
            "17  December 26, 2017       1  Positive  \n",
            "18  December 26, 2017       1  Negative  \n",
            "19  December 24, 2017       1  Negative  \n",
            "\n",
            "First 20 rows of 'fake_data' with sentiment analysis results:\n",
            "                                                title  \\\n",
            "0    Donald Trump Sends Out Embarrassing New Year’...   \n",
            "1    Drunk Bragging Trump Staffer Started Russian ...   \n",
            "2    Sheriff David Clarke Becomes An Internet Joke...   \n",
            "3    Trump Is So Obsessed He Even Has Obama’s Name...   \n",
            "4    Pope Francis Just Called Out Donald Trump Dur...   \n",
            "5    Racist Alabama Cops Brutalize Black Boy While...   \n",
            "6    Fresh Off The Golf Course, Trump Lashes Out A...   \n",
            "7    Trump Said Some INSANELY Racist Stuff Inside ...   \n",
            "8    Former CIA Director Slams Trump Over UN Bully...   \n",
            "9    WATCH: Brand-New Pro-Trump Ad Features So Muc...   \n",
            "10   Papa John’s Founder Retires, Figures Out Raci...   \n",
            "11   WATCH: Paul Ryan Just Told Us He Doesn’t Care...   \n",
            "12   Bad News For Trump — Mitch McConnell Says No ...   \n",
            "13   WATCH: Lindsey Graham Trashes Media For Portr...   \n",
            "14   Heiress To Disney Empire Knows GOP Scammed Us...   \n",
            "15   Tone Deaf Trump: Congrats Rep. Scalise On Los...   \n",
            "16   The Internet Brutally Mocks Disney’s New Trum...   \n",
            "17   Mueller Spokesman Just F-cked Up Donald Trump...   \n",
            "18   SNL Hilariously Mocks Accused Child Molester ...   \n",
            "19   Republican Senator Gets Dragged For Going Aft...   \n",
            "\n",
            "                                                 text subject  \\\n",
            "0   donald trump wish americans happy new year lea...    News   \n",
            "1   house intelligence committee chairman devin nu...    News   \n",
            "2   friday revealed former milwaukee sheriff david...    News   \n",
            "3   christmas day donald trump announced would bac...    News   \n",
            "4   pope francis used annual christmas day message...    News   \n",
            "5   number cases cops brutalizing killing people c...    News   \n",
            "6   donald trump spent good portion day golf club ...    News   \n",
            "7   wake yet another court decision derailed donal...    News   \n",
            "8   many people raised alarm regarding fact donald...    News   \n",
            "9   might thought get break watching people kiss d...    News   \n",
            "10  centerpiece donald trump campaign presidency w...    News   \n",
            "11  republicans working overtime trying sell scam ...    News   \n",
            "12  republicans seven years come viable replacemen...    News   \n",
            "13  media talking day trump republican party scam ...    News   \n",
            "14  abigail disney heiress brass ovaries profit go...    News   \n",
            "15  donald trump signed gop tax scam law course me...    News   \n",
            "16  new animatronic figure hall presidents walt di...    News   \n",
            "17  trump supporters socalled president favorite n...    News   \n",
            "18  right whole world looking shocking fact democr...    News   \n",
            "19  senate majority whip john cornyn rtx thought w...    News   \n",
            "\n",
            "                 date  label Sentiment  \n",
            "0   December 31, 2017      0  Positive  \n",
            "1   December 31, 2017      0  Positive  \n",
            "2   December 30, 2017      0  Negative  \n",
            "3   December 29, 2017      0  Positive  \n",
            "4   December 25, 2017      0  Positive  \n",
            "5   December 25, 2017      0  Negative  \n",
            "6   December 23, 2017      0  Negative  \n",
            "7   December 23, 2017      0  Negative  \n",
            "8   December 22, 2017      0  Positive  \n",
            "9   December 21, 2017      0  Positive  \n",
            "10  December 21, 2017      0  Negative  \n",
            "11  December 21, 2017      0  Negative  \n",
            "12  December 21, 2017      0  Positive  \n",
            "13  December 20, 2017      0  Positive  \n",
            "14  December 20, 2017      0  Positive  \n",
            "15  December 20, 2017      0  Positive  \n",
            "16  December 19, 2017      0  Positive  \n",
            "17  December 17, 2017      0  Positive  \n",
            "18  December 17, 2017      0  Negative  \n",
            "19  December 16, 2017      0  Positive  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Save the preprocessed data back to CSV files and display the first 20 rows of both True and Fake news Datasets**"
      ],
      "metadata": {
        "id": "ewuidOyc42AE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the directory path\n",
        "directory_path = '/content/drive/My Drive/Colab Notebooks/'\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(directory_path, exist_ok=True)"
      ],
      "metadata": {
        "id": "VLs5c6exeFMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_true_data_path = '/content/drive/My Drive/Colab Notebooks/first_20_true_data.csv'\n",
        "new_fake_data_path = '/content/drive/My Drive/Colab Notebooks/first_20_fake_data.csv'"
      ],
      "metadata": {
        "id": "h2GoUNIzelie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the first 20 rows of each dataset to new CSV files\n",
        "true_df.head(20).to_csv(new_true_data_path, index=False)\n",
        "fake_df.head(20).to_csv(new_fake_data_path, index=False)"
      ],
      "metadata": {
        "id": "vjr7z7BtH65k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nFirst 20 rows of 'true_data' saved to 'first_20_true_data.csv'.\")\n",
        "print(\"First 20 rows of 'fake_data' saved to 'first_20_fake_data.csv'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytBrwTUPH68J",
        "outputId": "abb733ad-96ff-43b0-b45a-eb298ea0eea2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "First 20 rows of 'true_data' saved to 'first_20_true_data.csv'.\n",
            "First 20 rows of 'fake_data' saved to 'first_20_fake_data.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Explain the features you used and your experiments**"
      ],
      "metadata": {
        "id": "TmuVi0fciqSi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Naive Bayes classifier with multi-fold cross-validation was used in this NLP project to classify text sentiment through a series of tests that utilized different feature sets. Our main objective was to investigate various feature engineering methods to improve sentiment analysis performance. An explanation of the features used and the studies performed is provided below:\n",
        "\n",
        "Text preparation\n",
        "\n",
        "In order to prepare the text data for feature extraction, we first implemented text preprocessing. The preprocessing procedures comprised:\n",
        "lowercase text transformation.\n",
        "punctuation is dropped in order to reduce noise.\n",
        "separating each word in the text into tokens.\n",
        "removing often used stop words, which are often uninformative.\n",
        "\n",
        "Function Sets:\n",
        "This baseline feature set was built using unigrams,\n",
        "which stand in for individual words. We tested a number of methods to improve the standard features, including:\n",
        "\n",
        "Stop Word Filtering: The quality of the features was improved by eliminating frequent stop words.\n",
        "Negation Handling: In order to modify emotion ratings based on the existence of negation terms, we took into account the negation context.\n",
        "Features of Sentiment Lexicon: We looked into sentiment lexicons, which assign words sentiment scores or counts. We thought about:\n",
        "Subjectivity Scores: Lexicons give words polarity or subjectivity scores. strong positive ratings influenced the overall mood, whereas strong negative numbers revealed the overall mood.\n",
        "Experiments and Evaluation: The following steps were used to systematically examine these feature sets:\n",
        "For sentiment categorization, we utilized the Naive Bayes classifier.\n",
        "We evaluated precision, recall, and F1-measure for each feature set to assess classification accuracy.\n",
        "For cross-validation, we used customized scoring with a focus on the F1-score.\n",
        "\n",
        "Through these tests, we evaluated the effectiveness of various feature sets and preprocessing methods. We were able to determine the best method for categorizing sentiment in text data thanks to the results. We were able to learn more about how feature engineering and text preprocessing affect the precision and dependability of sentiment analysis as a result of this procedure. It also highlighted the value of feature choice and the possible advantages of adding sentiment lexicons to improve the predictive capability of sentiment classifiers."
      ],
      "metadata": {
        "id": "azuxosVsgIGi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Explain how you examine whether the Fake content tends to contain more positive or negative sentences.**"
      ],
      "metadata": {
        "id": "ZFSQVL52jh9m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used sentiment analysis on a dataset of fake news items to determine if positive or negative sentences are more common in fake material. This is how we carried out the analysis:\n",
        "Sentimental Evaluation: I assigned sentiment scores to certain sentences within the fake news pieces using a sentiment analysis tool, such as VADER.\n",
        "Adding Up Scores: After examining each sentence, I added up the sentiment scores to find the article's overall tone. With the help of this compilation, we were able to categorize each article as \"Positive,\" \"Negative,\" or \"Neutral.\"\n",
        "Count and Comparison: I counted how many articles were classified as \"Positive\" and \"Negative.\" I was able to determine whether fake news items tended to have more positive or negative sentences by comparing these counts.\n",
        "This analysis revealed general sentiment trends in fake news content and revealed whether it leans more toward positive or negative sentiment. My understanding of the emotional undertone and potential biases in fake news articles has improved as a result of the findings.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6KIdzPOakrNX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Discuss what you have learned from this sentiment analysis assignment – it can be reflections on your methodology and process of doing this homework; or on the results you have obtained**"
      ],
      "metadata": {
        "id": "VqNU0jZylPKH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I've gained knowledge about the importance of feature engineering in text classification tasks thanks to this sentiment analysis assignment. I came to understand the importance of text preprocessing, which includes lowercasing, stop word removal, and punctuation handling, for raising the standard of features. The effect of feature selection on classification performance was demonstrated through experiments using various feature sets, including unigrams and bigrams. Additionally, adding sentiment lexicons improved the model's capacity for prediction. The procedure demonstrated how critical it is to assess classification results with precision, recall, and F1-score in order to balance positive and negative sentiment detection. The ability of sentiment analysis to glean insights from textual data was demonstrated, along with some of its drawbacks.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "b_22p9YnlfKJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***There are many datasets available for training on sentiment polarity. Below are some examples. Please choose one dataset for the training purpose and briefly explain why you choose it: ***\n",
        "• The sentence_polarity corpus introduced in class\n",
        "• http://www.cs.jhu.edu/~mdredze/datasets/sentiment/  \n",
        "• http://help.sentiment140.com/for-students\n",
        "• https://www.kaggle.com/crowdflower/twitter-airline-sentiment"
      ],
      "metadata": {
        "id": "zu33f-ELtU6_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For training on sentiment polarity, I would pick the \"Twitter Airline Sentiment\" dataset that is available on Kaggle.\n",
        "There are several reasons why this dataset is a popular option for sentiment analysis tasks:\n",
        "\n",
        "Real-World Data: This dataset for sentiment analysis is useful and real-world because it includes tweets about customer sentiment toward various airline companies.\n",
        "\n",
        "Large and Diverse: It provides a significant amount of data, which is essential for training robust sentiment classifiers, with over 14,000 labeled tweets.\n",
        "\n",
        "Three different sentiment classes—positive, negative, and neutral—are represented by the dataset's multiclass sentiment labels. When compared to binary sentiment datasets, this enables a more thorough analysis of sentiment.\n",
        "\n",
        "The dataset is hosted on Kaggle, which has a vibrant community.\n",
        "\n",
        "Variety of Text Data: The dataset's tweets' various lengths and levels of complexity offer a good variety of text data for training.\n",
        "\n",
        "Because of its size, diversity, and multiclass labels, the \"Twitter Airline Sentiment\" dataset is a well-rounded option for sentiment analysis training."
      ],
      "metadata": {
        "id": "1r9ma0OXtuf-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W6HtXfH4jzO8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}